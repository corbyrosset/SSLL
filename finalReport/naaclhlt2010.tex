%
% File naaclhlt2010.tex
%
% Contact: nasmith@cs.cmu.edu

\documentclass[11pt,letterpaper]{article}
\usepackage{naaclhlt2010}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
\usepackage{times}
\usepackage{latexsym}

\title{A Comparison of Stochastic Methods for PCA as Applied to Streaming Facial Recognition\Thanks{We would like to thank Dr. Raman Arora for his work on stochastic algorithms for manifold learning.}}

\author{Corbin Rosset\\
  Johns Hopkins University\\
  3400 N. Charles St.\\
  Baltimore, MD 21218, USA\\
  {\tt crosset2@jhu.edu}
  \And
  Edmund Duhaime \\
  Johns Hopkins University\\
  3400 N. Charles St.\\
  Baltimore, MD 21218, USA\\
  {\tt eduhaim1@jhu.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  This study applies a variety of stochastic and incremental techniques for principle component analysis (PCA) to quickly learn maximally informative linear subspaces on a streaming version of the Yale Face Data Set B. We compare the performance of a K-nearest neighbor classifier and a bagged tree learner on the best subspaces learned by each of: Stochastic Power Method (SPM), Incremental PCA (IPCA), Matrix Stochastic Descent (MSG), and Sparse PCA (SPCA). We compare and contrast the theoretical properties such as correctness, space, and iteration complexity. In all cases, the memory required to store a subspace capable of achieving accuracy similar to that of the baseline classifier was  2-4$\%$ of the size of the input data. These algorithms play an important role in improving the scalability of facial recognition in a streaming setting. 
\end{abstract}

\section{Introduction}

The premise of subspace learning is to map a data matrix $X \in \mathcal{R}^{d \times n}$ of $n$ examples each of dimension $d$ to a $k$ dimensional subspace, $k << d$ that preserves maximal ``information''. The motivation is that most data sets capture redundant, verbose, or noisy features that dilute the true parameters behind the data.  



\section{PCA and its Stochastic Variants}

introduce dual but equivalent concepts of reconstruction error and variance retention

state the empirical covariance matrix is d x d too big to be in memory

show how to solve for PCA using power method if true covariance were known



\subsection{Stochastic Power Method}

notes from class and relation to power method


\subsection{Incremental PCA}
notes from class


\subsection{Matrix Stochastic Gradient}

notes from class

\subsection{Sparse PCA}

notes from class


\section{K-NN for High-Dimensional Data}

discuss johnson lindenstrauss lemma popularly applied to proximity problems to preserve pairwise distances in a lower dimensional space

\section{Experiments}

{\bf NED: describe the contents of the data, how we removed bad images, just like proposal...} 

\section{Results}

{\bf NED: pictures of 1) reconstructed faces with 5, 10, 15... principle components 
2) table of the best dimension and associated accuracy achieved by each algorithm 
3) some examples of ``eigenfaces'' (the top five principle components shown as images) as well as their thresholded versions. You'll find these in the figures directory. 
4) graphs of accuracy versus number of principle components for some of the algorithms (choose the better looking ones)}

\begin{table}
\begin{center}
\begin{tabular}{|l|rl|}
\hline \bf Type of Text & \bf Font Size & \bf Style \\ \hline
paper title & 15 pt & bold \\
author names & 12 pt & bold \\
author affiliation & 12 pt & \\
the word ``Abstract'' & 12 pt & bold \\
section titles & 12 pt & bold \\
document text & 11 pt  &\\
abstract text & 10 pt & \\
captions & 10 pt & \\
bibliography & 10 pt & \\
footnotes & 9 pt & \\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} Font guide. }
\end{table}

{\bf Captions}: Provide a caption for every illustration; number each one
sequentially in the form:  ``Figure 1. Caption of the Figure.'' ``Table 1.
Caption of the Table.''  Type the captions of the figures and 
tables below the body, using 10 point text.  

\section{Conclusion}

all algorithms for pca should learn the same subspace, up to small rotations and scalings...

discuss applications of streaming subspace learning

there are extensions for incremental kernal pca and dealing with the case of missing data...

\section*{Acknowledgments}

Do not number the acknowledgment section.

\begin{thebibliography}{}

%%cite these:

 \bibitem{@Article{KCLee05, 
  author =  "K.C. Lee and J. Ho and D. Kriegman", 
  title =   "Acquiring Linear Subspaces for Face Recognition under Variable Lighting ", 
  journal = "IEEE Trans. Pattern Anal. Mach. Intelligence", 
  year =  2005, 
  volume = 27, 
  number = 5, 
  pages= "684-698"} }

the following are examples

\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
Alfred~V. Aho and Jeffrey~D. Ullman.
\newblock 1972.
\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
\newblock Prentice-{Hall}, Englewood Cliffs, NJ.

\bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
{American Psychological Association}.
\newblock 1983.
\newblock {\em Publications Manual}.
\newblock American Psychological Association, Washington, DC.

\bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
{Association for Computing Machinery}.
\newblock 1983.
\newblock {\em Computing Reviews}, 24(11):503--512.

\bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
\newblock 1981.
\newblock Alternation.
\newblock {\em Journal of the Association for Computing Machinery},
  28(1):114--133.

\bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
Dan Gusfield.
\newblock 1997.
\newblock {\em Algorithms on Strings, Trees and Sequences}.
\newblock Cambridge University Press, Cambridge, UK.

\end{thebibliography}

\end{document}
